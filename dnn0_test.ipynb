{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.vae import VAE\n",
    "from models.rnn import RNN\n",
    "from models.dnn import DNN\n",
    "\n",
    "import re\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from scipy.io import loadmat\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "from pandas import DataFrame, concat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 13\n",
    "# seed = 23\n",
    "torch.manual_seed(seed)\n",
    "# torch.cuda.manual_seed(seed)\n",
    "# torch.cuda.manual_seed_all(seed)\n",
    "# torch.backends.cudnn.deterministic = True\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to all dataset\n",
    "DATASET_PATH = os.path.join(os.getcwd(), 'data')\n",
    "# Path to the folder where the pretrained models are saved\n",
    "CHECKPOINT_PATH = os.path.join(os.getcwd(), 'checkpoints')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **DNN Data Generation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def series_to_supervised(data, n_in=1, n_out=1, dropnan=True):\n",
    "    n_vars = 1 if type(data) is list else data.shape[1]\n",
    "    df = DataFrame(data)\n",
    "    cols, names = list(), list()\n",
    "    # input sequence (t-n, ... t-1)\n",
    "    for i in range(n_in, 0, -1):\n",
    "        cols.append(df.shift(i))\n",
    "        names += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    # forecast sequence (t, t+1, ... t+n)\n",
    "    for i in range(0, n_out):\n",
    "        cols.append(df.shift(-i))\n",
    "        if i == 0:\n",
    "            names += [('var%d(t)' % (j+1)) for j in range(n_vars)]\n",
    "        else:\n",
    "            names += [('var%d(t+%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    # put it all together\n",
    "    agg = concat(cols, axis=1)\n",
    "    agg.columns = names\n",
    "    # drop rows with NaN values\n",
    "    if dropnan:\n",
    "        agg.dropna(inplace=True)\n",
    "    return agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading data ... : 100%|██████████| 5/5 [00:01<00:00,  2.68it/s]\n"
     ]
    }
   ],
   "source": [
    "Z, RES = defaultdict(dict), defaultdict(dict)\n",
    "for intensity in tqdm(range(5,10), desc='Reading data ... '):\n",
    "    ATTACK_DATA_PATH = os.path.join(DATASET_PATH, f'I{intensity}')\n",
    "    for filename in os.listdir(ATTACK_DATA_PATH):\n",
    "        fname = os.path.join(ATTACK_DATA_PATH, filename)\n",
    "        data = loadmat(fname)\n",
    "        att_type = int(re.findall(r'\\d+', filename)[0])\n",
    "        Z[intensity][att_type] = data['Z']\n",
    "        RES[intensity][att_type] = data['Res']\n",
    "\n",
    "NORMAL_DATA_FILENAME = 'Normal2.mat'\n",
    "data = loadmat(os.path.join(DATASET_PATH, NORMAL_DATA_FILENAME))\n",
    "Z[0] = data['Z']\n",
    "RES[0] = data['Res']\n",
    "\n",
    "\n",
    "Z = dict(sorted(Z.items()))\n",
    "RES = dict(sorted(RES.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_dim = 118\n",
    "hidden_size = 100\n",
    "num_layers = 2\n",
    "rnn_window = 10\n",
    "samples = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "noise_levels = [5, 10, 20, 50]\n",
    "vaes, rnns = dict(), dict()\n",
    "for level in noise_levels:\n",
    "    vae = VAE(latent_dim).to(device)\n",
    "    vae.load_state_dict(torch.load(os.path.join(CHECKPOINT_PATH, f'vae_{level}%.pt')))\n",
    "    vaes[level] = vae\n",
    "\n",
    "    rnn = RNN(input_dims=rnn_window * latent_dim,\n",
    "              outputdims=latent_dim,\n",
    "              hidden_size=hidden_size,\n",
    "              num_layers=num_layers).to(device)\n",
    "    rnn.load_state_dict(torch.load(os.path.join(CHECKPOINT_PATH, f'rnn_{level}%.pt')))\n",
    "    rnns[level] = rnn\n",
    "\n",
    "dnn = DNN().to(device)\n",
    "dnn.load_state_dict(torch.load(os.path.join(CHECKPOINT_PATH, 'dnn.pt')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "Res = defaultdict(dict)\n",
    "for level in tqdm(noise_levels, desc=' Generating residuals ... '):\n",
    "    vae = vaes[level].eval()\n",
    "    rnn = rnns[level].eval()\n",
    "    for intensity in range(5,10):\n",
    "        for k, v in Z[intensity].items():\n",
    "            # state estimation residual\n",
    "            r1 = torch.tensor(RES[intensity][k], dtype=torch.float32)\n",
    "            z = torch.tensor(v, dtype=torch.float32).to(device)\n",
    "            R = []\n",
    "            for _ in range(samples):\n",
    "                # VAE residuals\n",
    "                z_rec = vae(z)\n",
    "                r2 = (z_rec - z).detach().cpu()\n",
    "                \n",
    "                # rnn prediction residual\n",
    "                s_t = vae.encoder(z)\n",
    "                s_t = series_to_supervised(s_t.cpu().detach().numpy(), n_in=rnn_window).values[:,:rnn_window * latent_dim]\n",
    "                s_t = torch.tensor(s_t, dtype=torch.float32)\n",
    "                s_tt = rnn(s_t)\n",
    "                z_rnn = vae.decoder(s_tt)\n",
    "                r3 = (z_rnn - z[rnn_window:]).detach().cpu()\n",
    "                R.append(torch.stack([r1[rnn_window:],r2[rnn_window:],r3], dim=1))\n",
    "            Res[level][k] = torch.stack(R, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
